{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJSeAgdrRdMo3ZCxtGcELj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirsh008/reinforcement-learning-model-training/blob/main/cart_pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "dItqA1miYII-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dbgFlm8RYYAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV_qzbdz5WdF"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, s_size, a_size, h_size):\n",
        "    super(Policy, self).__init__()\n",
        "    self.fc1 = nn.Linear(s_size, h_size)\n",
        "    self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x, dim=1)\n",
        "\n",
        "  def act(self, state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    probs = self.forward(state).cpu()\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "bLWUT0hbL7DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "  scores_deque = deque(maxlen= 100)\n",
        "  scores = []\n",
        "  for i_episode in range(1, n_training_episodes + 1):\n",
        "    saved_log_prob = []\n",
        "    rewards = []\n",
        "    state = env.reset()[0]\n",
        "    for t in range(max_t):\n",
        "      action, log_prob = policy.act(state)\n",
        "      saved_log_prob.append(log_prob)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      rewards.append(reward)\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "    scores_deque.append(sum(rewards))\n",
        "    scores.append(sum(rewards))\n",
        "\n",
        "    returns = deque(maxlen=max_t)\n",
        "    n_steps = len(rewards)\n",
        "\n",
        "    for t in range(n_steps)[::-1]:\n",
        "      disc_return_t = rewards[t] + (gamma * disc_return_t if t < n_steps - 1 else 0)\n",
        "      returns.appendleft(disc_return_t)\n",
        "\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    policy_loss = []\n",
        "    for log_prob, disc_return in zip(saved_log_prob, returns):\n",
        "      policy_loss.append(-log_prob * disc_return)\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "  return scores"
      ],
      "metadata": {
        "id": "MFmMritMNyEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\" : 64,\n",
        "    \"n_training_episodes\" : 1000,\n",
        "    \"n_evaluation_episodes\" : 10,\n",
        "    \"max_t\" : 1000,\n",
        "    \"gamma\" : 1.0,\n",
        "    \"lr\" : 1e-3,\n",
        "    \"env_id\" : env_id,\n",
        "    \"state_space\" : s_size,\n",
        "    \"action_space\" : a_size,\n",
        "}"
      ],
      "metadata": {
        "id": "1_iUIYuzSbhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ],
      "metadata": {
        "id": "361s7rjWTUq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"],\n",
        "                   100)"
      ],
      "metadata": {
        "id": "pSoJ8OqAU54q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  episode_rewards = []\n",
        "  for episodes in range(n_eval_episodes):\n",
        "    state = env.reset()[0] # Modified: get only the observation\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "_POqVJOZVNlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_agent(eval_env,\n",
        "               cartpole_hyperparameters[\"max_t\"],\n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)\n",
        "\n",
        "print(f\"Mean reward: {mean_reward:.2f}\")\n",
        "print(f\"Standard deviation of reward: {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0GOmQJWfJK",
        "outputId": "f2239011-2466-431c-99dd-fbaf7c5cc0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 500.00\n",
            "Standard deviation of reward: 0.00\n"
          ]
        }
      ]
    }
  ]
}