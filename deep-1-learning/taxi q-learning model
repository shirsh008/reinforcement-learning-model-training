deep-1-learning/
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyQ4UbPd8oQ9+IQHf+8eyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirsh008/reinforcement-learning-model-training/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"Taxi-v3\", render_mode = \"ansi\")"
      ],
      "metadata": {
        "id": "IS5Ztl9yQYPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "\n",
        "  def __init__(self, env, alpha, gamma):\n",
        "\n",
        "    self.env = env\n",
        "\n",
        "    self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def get_action(self,state):\n",
        "\n",
        "    return np.argmax(self.q_table[state])\n",
        "\n",
        "  def update_parameters(self, state, action, reward, next_state):\n",
        "\n",
        "    old_value = self.q_table[state,action]\n",
        "    next_max = np.max(self.q_table[next_state])\n",
        "    new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "\n",
        "    self.q_table[state,action] = new_value"
      ],
      "metadata": {
        "id": "fqaNGVwCQq59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "agent = QLearningAgent(env,alpha,gamma)"
      ],
      "metadata": {
        "id": "fEo-8DScgNTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "epsilon = 0.1\n",
        "\n",
        "n_episodes = 10000\n",
        "\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "for i in tqdm(range(n_episodes)):\n",
        "\n",
        "  # env.reset() returns (observation, info). Extract the integer state.\n",
        "  obs, info_reset = env.reset()\n",
        "  state = obs # For Taxi-v3, obs itself is the integer state.\n",
        "              # If obs was a tuple like (192, ...), we'd use obs[0]\n",
        "\n",
        "  epoches, penalties, reward = 0,0,0\n",
        "  terminated = False\n",
        "  truncated = False # Initialize truncated for Gymnasium 0.28+\n",
        "\n",
        "  while not terminated and not truncated: # Loop until episode is terminated or truncated\n",
        "\n",
        "    if random.uniform(0,1) < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = agent.get_action(state)\n",
        "\n",
        "    # env.step() returns (observation, reward, terminated, truncated, info)\n",
        "    next_obs, reward, terminated, truncated, info_step = env.step(action)\n",
        "    next_state = next_obs # For Taxi-v3, next_obs itself is the integer state.\n",
        "                          # If next_obs was a tuple like (192, ...), we'd use next_obs[0]\n",
        "\n",
        "    agent.update_parameters(state, action, reward, next_state)\n",
        "\n",
        "    if reward == -10:\n",
        "      penalties += 1\n",
        "\n",
        "    state = next_state\n",
        "    epoches += 1\n",
        "\n",
        "  timesteps_per_episode.append(epoches)\n",
        "  penalties_per_episode.append(penalties)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmseJsk2gZcf",
        "outputId": "f969a150-84cb-45ce-ef82-fa37cf8224eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:13<00:00, 766.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = 223\n",
        "\n",
        "env.unwrapped.s = state\n",
        "\n",
        "epoches, reward, penalties = 0,0,0\n",
        "\n",
        "frames = []\n",
        "\n",
        "terminated = False\n",
        "truncated = False\n",
        "\n",
        "while not terminated and not truncated:\n",
        "\n",
        "  action = agent.get_action(state)\n",
        "\n",
        "  next_state, reward, terminated, truncated, info_step = env.step(action)\n",
        "\n",
        "  if reward == -10:\n",
        "    penalties += 1\n",
        "\n",
        "  frames.append({\n",
        "      'frame' : env.render(),\n",
        "      'state' : state,\n",
        "      'action' : action,\n",
        "      'reward' : reward\n",
        "  })\n",
        "\n",
        "  state = next_state\n",
        "  epoches += 1\n",
        "\n",
        "print(\"Timesteps taken: {}\".format(epoches))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt4cb3wFiQb8",
        "outputId": "2b879cb2-f716-45df-cb87-7afb95a892fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 12\n",
            "Penalties incurred: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(frame['frame'])\n",
        "    print(\"timestep : \", i+1, \" of \", len(frames))\n",
        "    print(\"state : \", frame['state'])\n",
        "    print(\"action : \", frame['action'])\n",
        "    print(\"reward : \", frame['reward'])\n",
        "    sleep(0.5)\n",
        "\n",
        "print_frames(frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NtdnA_m4L9o",
        "outputId": "73f29953-2326-45ab-aa5f-417b13291625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "timestep :  11  of  12\n",
            "state :  379\n",
            "action :  0\n",
            "reward :  -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epsilon = 0.1\n",
        "n_episodes = 100\n",
        "\n",
        "timesteps_per_episode = []\n",
        "penalties_per_episode = []\n",
        "\n",
        "for i in tqdm(range(0,n_episodes)):\n",
        "  obs, info_step = env.reset()\n",
        "  state = obs\n",
        "\n",
        "  epoches, penalties, reward = 0, 0, 0\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "\n",
        "  while not terminated and not truncated:\n",
        "\n",
        "    action = agent.get_action(state)\n",
        "\n",
        "    next_state, reward, terminated, truncated, info_step = env.step(action)\n",
        "\n",
        "    agent.update_parameters(state, action, reward, next_state)\n",
        "\n",
        "    if reward == -10:\n",
        "      penalties += 1\n",
        "\n",
        "    state = next_state\n",
        "    epoches += 1\n",
        "\n",
        "  timesteps_per_episode.append(epoches)\n",
        "  penalties_per_episode.append(penalties)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaien-VNxxoX",
        "outputId": "7987060f-c918-4eb6-8c61-c44b351f041b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 2789.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Aevrage timesteps taken to complete the ride : \", sum(timesteps_per_episode)/n_episodes)\n",
        "print(\"Aevrage penalties taken to complete the ride : \", sum(penalties_per_episode)/n_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0kSSuRc0WHz",
        "outputId": "7414e700-f02f-4bbb-99a1-5fb8369a7da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aevrage timesteps taken to complete the ride :  13.05\n",
            "Aevrage penalties taken to complete the ride :  0.0\n"
          ]
        }
      ]
    }
  ]
}
